{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f5bb5-11a3-447d-b607-d56742d5ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe7a8b7-731c-447f-b0f7-4214d563b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jamie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jamie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "# downloading the nltk data for preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# downloading the data\n",
    "data_urls = ['https://raw.githubusercontent.com/KaliaBarkai/KaggleDisasterTweets/master/Data/%s.csv'%ds for ds in ['train', 'test', 'sample_submission']]\n",
    "\n",
    "# reading the data as pandas dataframe\n",
    "train = pd.read_csv(data_urls[0])\n",
    "test = pd.read_csv(data_urls[1])\n",
    "sample_submission = pd.read_csv(data_urls[2])\n",
    "\n",
    "# NLP pre-processing\n",
    "# remove urls, handles, and the hashtag from hashtags \n",
    "# (taken from https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression)\n",
    "def remove_urls(text):\n",
    "  new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "  return new_text\n",
    "\n",
    "# make all text lowercase\n",
    "def text_lowercase(text): \n",
    "  return text.lower()\n",
    "\n",
    "# remove numbers\n",
    "def remove_numbers(text): \n",
    "  result = re.sub(r'\\d+', '', text) \n",
    "  return result\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text): \n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  return text.translate(translator)\n",
    "\n",
    "# function for all pre-processing steps\n",
    "def preprocessing(text):\n",
    "  text = text_lowercase(text)\n",
    "  text = remove_urls(text)\n",
    "  text = remove_numbers(text)\n",
    "  text = remove_punctuation(text)\n",
    "  return text\n",
    "\n",
    "# pre-processing the text body column\n",
    "pp_text = []\n",
    "for text_data in train['text']:\n",
    "  # check if string\n",
    "  if isinstance(text_data, str):\n",
    "    pp_text_data = preprocessing(text_data)\n",
    "    pp_text.append(pp_text_data)\n",
    "   # if not string\n",
    "  else:\n",
    "    pp_text.append(np.NaN)\n",
    "\n",
    "# add pre-processed column to dataset\n",
    "train['pp_text'] = pp_text\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[\"pp_text\"], train[\"target\"])\n",
    "\n",
    "# create bag-of-words with weights using tfid vectoriser\n",
    "# strip accents and remove stop words during vectorisation\n",
    "tf=TfidfVectorizer(strip_accents = 'ascii', stop_words='english')\n",
    "\n",
    "# transform and fit the training set with vectoriser\n",
    "X_train_tf = tf.fit_transform(X_train)\n",
    "# transform the test set with vectoriser\n",
    "X_test_tf = tf.transform(X_test)\n",
    "\n",
    "# create logistic regression model\n",
    "logreg = LogisticRegression(verbose=1, random_state=0, penalty='l2', solver='newton-cg')\n",
    "# train model on  vectorised training data\n",
    "model = logreg.fit(X_train_tf, y_train)\n",
    "# evaluate model performance on the test set\n",
    "pred = model.predict(X_test_tf)\n",
    "#metrics.f1_score(y_test, pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5707ac3-892a-4c79-9129-e72ddd36ac3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this earthquake ma...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3         people receive wildfires evacuation orders i...\n",
       "4       just got sent this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    ahrary the out of control wild fires in califo...\n",
       "7610                      m    utc km s of volcano hawaii\n",
       "7611    police investigating after an e bike collided ...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: pp_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c11109-b604-4c9d-b90c-0d55448aec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b39d164-8221-459b-9f85-3e1dc4e2cc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5709x11936 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 45540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347f7347-eeec-4b9b-a14e-09ecef3f5257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce5f3d8-1cb6-4f79-8cd4-53f3d4510750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5709x11965 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 45708 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68efb749-d832-4fb9-9b37-3040099c07b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3612dd-5d8a-461c-bddf-57d2a0325965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import lime\n",
    "import sklearn.ensemble\n",
    "from __future__ import print_function\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# converting the vectoriser and model into a pipeline\n",
    "# this is necessary as LIME takes a model pipeline as an input\n",
    "c = make_pipeline(tf, model)\n",
    "\n",
    "# saving a list of strings version of the X_test object\n",
    "ls_X_test= list(X_test)\n",
    "\n",
    "# saving the class names in a dictionary to increase interpretability\n",
    "class_names = {0: 'non-disaster', 1:'disaster'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd33b688-199f-451d-8113-18677ec4d3dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3066/1170743.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# use the probability results of the logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# can also add num_features parameter to reduce the number of features explained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mLIME_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLIME_explainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_X_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document id: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# create the LIME explainer\n",
    "# add the class names for interpretability\n",
    "LIME_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "# choose a random single prediction\n",
    "idx = 15\n",
    "# explain the chosen prediction \n",
    "# use the probability results of the logistic regression\n",
    "# can also add num_features parameter to reduce the number of features explained\n",
    "LIME_exp = LIME_explainer.explain_instance(ls_X_test[idx], c.predict_proba)\n",
    "# print results\n",
    "print('Document id: %d' % idx)\n",
    "print('Tweet: ', ls_X_test[idx])\n",
    "print('Probability disaster =', c.predict_proba([ls_X_test[idx]]).round(3)[0,1])\n",
    "print('True class: %s' % class_names.get(list(y_test)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "427f3faa-9ad5-43a5-9dd8-1fcbec450b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c494d1-fa47-4bf0-8c04-4f5d9c5a6c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap_vals' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3066/1793445221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_vals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shap_vals' is not defined"
     ]
    }
   ],
   "source": [
    "shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f34071-1471-40b0-8daf-96a49f7f37ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
